--- 
title: "UKBAnalytica: A Guide to UK Biobank Data Analysis"
author:
- name: "Nan He"
  email: "hinna@i.smu.edu.cn"
  affiliation: "Department of Bioinformatics, School of Basic Medical Sciences, Southern Medical University"

date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: []
description: |
  A comprehensive guide to the UKBAnalytica R package for processing UK Biobank
  Research Analysis Platform (RAP) data, including data download, preprocessing,
  disease phenotyping, survival analysis, and multiple imputation.
link-citations: yes
github-repo: Hinna0818/UKBAnalytica
---

# Introduction {#intro}
![package-overview](https://raw.githubusercontent.com/Hinna0818/UKBAnalytica/main/man/figures/package-overview.png)
**UKBAnalytica** is a high-performance R package for processing UK Biobank (UKB)
Research Analysis Platform (RAP) data exports. It provides efficient extraction
of diagnosis records from multiple sources and generates Cox regression-ready
survival datasets.

## Key features

- Built on `data.table` for efficient processing of large-scale biobank data.
- Supports ICD-10, ICD-9, self-reported illness, and death registry data.
- Flexible dual-source case definitions for main and sensitivity analyses.
- Generates survival datasets with proper prevalent/incident case handling.
- Includes predefined definitions for common cardiovascular and metabolic diseases.
- Provides standardized preprocessing for common UKB baseline variables.

## Installation

Install the development version from GitHub:

```r
# install.packages("devtools")
devtools::install_github("Hinna0818/UKBAnalytica")
```

## Package overview

The package covers the following workflow:

1. **Data download** -- Python scripts for downloading data from the RAP platform.
2. **Variable preprocessing** -- Standardized cleaning and recoding of baseline characteristics.
3. **Disease phenotyping** -- Predefined and custom disease definitions from multiple coding systems.
4. **Survival analysis** -- Build Cox-ready datasets with proper prevalent/incident classification.
5. **Baseline tables** -- Create Table 1 summaries stratified by case/control status.
6. **Multiple imputation** -- Impute missing covariates and merge results back to the full dataset.

The remaining chapters walk through each step with code examples.

## Core functions

| Function | Description |
|:---------|:------------|
| `build_survival_dataset()` | Build wide-format survival dataset |
| `extract_cases_by_source()` | Extract cases from specified data sources |
| `get_predefined_diseases()` | Get predefined disease definitions |
| `create_disease_definition()` | Create a custom disease definition |
| `preprocess_baseline()` | Preprocess baseline characteristics |
| `create_baseline_table()` | Create a Table 1 (via `tableone`) |
| `run_imputation()` | Multiple imputation with `mice` |

<!--chapter:end:index.Rmd-->

# Downloading Data from RAP {#rap-download}

The UKBAnalytica package ships Python helper scripts for downloading data from the
UK Biobank Research Analysis Platform (RAP). These scripts live in `inst/python/`
after package installation.

## File structure

```
inst/
  python/
    ukb_data_loader.py          # Demographics & metabolites (Spark)
    protein_loader.py           # Proteomics (dx commands)
    field_ids_demographic.txt   # Example demographic field IDs
  extdata/
    metabolites_non_ratio.txt   # Non-ratio metabolite reference (170 fields)
```

## Demographic data

Download any combination of UKB fields by specifying their field IDs.
The loader uses Spark via `dxdata` under the hood.

```bash
# Pass IDs directly
python ukb_data_loader.py demographic \
  --ids 31,53,21022,21001 \
  -o population.csv

# Or read IDs from a file (recommended for many fields)
python ukb_data_loader.py demographic \
  --id-file field_ids_demographic.txt \
  -o population.csv
```

The ID file supports comments (`#`), comma-separated and space-separated formats:

```txt
# Demographics
31      # Sex
53      # Date of assessment
21022   # Age at recruitment

# Comma separated
20116, 20117, 1289
```

## Metabolomics data (NMR)

Download NMR metabolomics data. You can retrieve all 251 metabolite fields
or restrict to the curated non-ratio subset of 170 fields.

```bash
# All metabolite fields
python ukb_data_loader.py metabolites -o metabolites_all.csv

# Non-ratio subset only
python ukb_data_loader.py metabolites \
  --non-ratio \
  -o metabolites_non_ratio.csv
```

## Proteomics data (Olink)

Download Olink protein expression data via `dx` commands.
The loader handles batching, merging, and progress tracking automatically.

```bash
# Default settings
python protein_loader.py -o ./protein_data

# Custom batch size and delay
python protein_loader.py \
  -o ./protein_data \
  --batch-size 100 \
  --delay 3

# Skip merging batch files
python protein_loader.py -o ./protein_data --no-merge
```

## Common UKB field IDs

The table below lists commonly used field IDs for reference:

| Category | Field IDs | Description |
|:---------|:----------|:------------|
| Basic demographics | 31, 53, 21022, 21001 | Sex, assessment date, age, BMI |
| Lifestyle | 20116, 20117, 1160 | Smoking, alcohol, sleep |
| Blood pressure | 93, 94, 4079, 4080 | Systolic/diastolic BP |
| Biomarkers | 30870, 30780, 30760, 30750 | Triglycerides, LDL, HDL, glucose |
| Hospital records | 41270, 41280, 41271, 41281 | ICD-10/9 diagnoses + dates |
| Death registry | 40000, 40001, 40002 | Death date, causes |

## Typical output sizes

| Data type | File | Approx. size | Rows | Columns |
|:----------|:-----|:-------------|:-----|:--------|
| Demographics | population.csv | 10--500 MB | ~500 K | variable |
| Metabolites (all) | metabolites.csv | ~300 MB | ~120 K | 251 |
| Metabolites (non-ratio) | metabolites_non_ratio.csv | ~200 MB | ~120 K | 170 |
| Proteomics | protein_all_merged.csv | ~800 MB | ~50 K | ~3000 |

## Troubleshooting

- **Spark initialisation fails**: make sure you are running inside a RAP JupyterLab session.
- **Field not found**: some fields require instance/array suffixes (e.g., `_i0`, `_a0`).
- **Memory issues**: reduce `--batch-size` for protein downloads.
- **Timeout**: increase `--delay`.

<!--chapter:end:01-rap-download.Rmd-->

# Variable Preprocessing {#preprocessing}

After downloading data from RAP, the next step is to clean and recode baseline
variables. UKBAnalytica provides `preprocess_baseline()` and several helper
functions to handle this in a standardised way.

## Available variables

Use `get_variable_info()` to inspect the built-in variable catalogue:

```r
library(UKBAnalytica)

# View all variables in a category
get_variable_info("demographics")
get_variable_info("lifestyle")
get_variable_info("biomarkers")
```

Each entry maps a human-readable name (e.g., `"bmi"`) to the corresponding UKB
field column (e.g., `p21001_i0`).

## Basic usage

`preprocess_baseline()` selects, renames, and recodes the requested variables:
  
```r
library(UKBAnalytica)
library(data.table)

ukb_data <- fread("population.csv")

processed <- preprocess_baseline(
  ukb_data,
  variables = c("sex", "age", "ethnicity", "bmi",
                "smoking", "education"),
  missing_action = "keep"
)
head(processed)
```

Set `missing_action = "drop"` to remove any row that has `NA` in the selected
variables.

## Blood pressure

UKB records automated and manual BP readings. The helper
`calculate_blood_pressure()` averages available readings per participant:

```r
processed <- calculate_blood_pressure(processed, type = "sbp")
processed <- calculate_blood_pressure(processed, type = "dbp")
```

This will create columns `sbp` and `dbp` from whichever readings are present
(automated preferred, manual as fallback).

## Medication use

Extract binary indicators for medication use at baseline:

```r
processed <- extract_medications(
  processed,
  c("cholesterol", "blood_pressure", "insulin")
)
```

The resulting columns are named `anti_cho`, `antihypertensive`, and `insulin`
(1 = taking medication, 0 = not).

## Air pollution exposure

Calculate averaged air pollution exposures from the UKB environmental data:

```r
processed <- calculate_air_pollution(
  processed,
  c("NO2", "PM2.5", "PM10", "NOx")
)
```

## Custom variable mapping

If you need a UKB field that is not in the built-in catalogue, pass a
`custom_mapping` list:

```r
custom <- list(
  my_biomarker = list(
    ukb_col = "p30000_i0",
    description = "Custom biomarker"
  )
)

processed <- preprocess_baseline(
  ukb_data,
  variables = c("sex", "age", "my_biomarker"),
  custom_mapping = custom
)
```

The custom entry will be treated like any built-in variable -- the column is
renamed and included in the output.

## Complete preprocessing example

A typical preprocessing pipeline for a cardiovascular study might look like
this:

```r
library(UKBAnalytica)
library(data.table)

dt <- fread("population.csv")

# Step 1: demographics + lifestyle + biomarkers
dt <- preprocess_baseline(
  dt,
  variables = c(
    "sex", "age", "ethnicity", "bmi",
    "smoking", "drinking",
    "sleep_duration", "exercise_intensity",
    "education", "income",
    "triglycerides", "ldl", "hdl", "hba1c"
  )
)

# Step 2: blood pressure
dt <- calculate_blood_pressure(dt, "sbp")
dt <- calculate_blood_pressure(dt, "dbp")

# Step 3: medications
dt <- extract_medications(
  dt, c("cholesterol", "blood_pressure", "insulin")
)

# Step 4: air pollution
dt <- calculate_air_pollution(dt, c("NO2", "PM2.5"))

str(dt)
```

<!--chapter:end:02-preprocessing.Rmd-->

# Disease Definitions {#disease-definitions}

UKBAnalytica identifies disease cases from up to four data sources:

| Source | Code field | Date field |
|:-------|:-----------|:-----------|
| ICD-10 (hospital) | `p41270` | `p41280_a*` |
| ICD-9 (hospital) | `p41271` | `p41281_a*` |
| Self-report | `p20002_i*_a*` | `p20008_i*_a*` |
| Death registry | `p40001`, `p40002` | `p40000` |

## Predefined diseases

The package ships a curated library of definitions for common cardiovascular
and metabolic conditions. Retrieve them with `get_predefined_diseases()`:

```r
library(UKBAnalytica)

diseases <- get_predefined_diseases()
names(diseases)
```

Select a subset by name:

```r
my_diseases <- get_predefined_diseases()[
  c("AA", "Hypertension", "Diabetes")
]
```

Each definition is a list specifying ICD-10 codes, ICD-9 codes, self-report
codes, and death-cause codes.

## Inspecting a definition

```r
diseases <- get_predefined_diseases()
str(diseases[["Hypertension"]])
```

A definition typically contains:

- `icd10` -- character vector of ICD-10 codes (prefix-matched).
- `icd9` -- character vector of ICD-9 codes.
- `self_report` -- integer vector of self-report coding IDs.
- `death_icd10` -- character vector of death-cause ICD-10 codes.

## Creating custom definitions

Use `create_disease_definition()` to define a new disease:

```r
my_disease <- create_disease_definition(
  icd10       = c("K70", "K71", "K72", "K73", "K74"),
  icd9        = c("571"),
  self_report = c(1604),
  death_icd10 = c("K70", "K74")
)
```

You can combine it with predefined diseases:

```r
all_diseases <- c(
  get_predefined_diseases()[c("Hypertension", "Diabetes")],
  list(LiverDisease = my_disease)
)
```

## Composite endpoints

Create composite endpoints by merging multiple definitions:

```r
mace <- combine_disease_definitions(
  get_predefined_diseases()[c("MI", "Stroke", "HF")]
)
```

This merges ICD-10, ICD-9, self-report, and death codes from all input
definitions into a single combined definition.

## Comparing data sources

To check how many cases each source contributes, use
`compare_data_sources()`:

```r
library(data.table)
ukb_data <- fread("population.csv")
diseases <- get_predefined_diseases()[c("AA", "Hypertension")]

comparison <- compare_data_sources(ukb_data, diseases)
comparison
```

This returns a summary table showing case counts per source per disease,
which is useful for justifying source selection in your manuscript.

<!--chapter:end:03-disease-definitions.Rmd-->

# Survival Analysis {#survival-analysis}

The main function for building a Cox-regression-ready dataset is
`build_survival_dataset()`. It handles prevalent/incident classification,
follow-up time calculation, and supports separate source definitions for
baseline exclusion and outcome ascertainment.

## Quick start

```r
library(UKBAnalytica)
library(data.table)

ukb_data <- fread("population.csv")

diseases <- get_predefined_diseases()[
  c("AA", "Hypertension", "Diabetes")
]

analysis_dt <- build_survival_dataset(
  dt = ukb_data,
  disease_definitions = diseases,
  primary_disease = "AA",
  censor_date = as.Date("2023-12-31")
)

head(analysis_dt[, .(eid, AA_history, AA_incident,
                      Hypertension_history, Diabetes_history,
                      outcome_status, outcome_surv_time)])
```

## Output columns

The wide-format output contains the following columns for each disease:

| Column | Description |
|:-------|:------------|
| `{Disease}_history` | 1 if prevalent (diagnosed at or before baseline) |
| `{Disease}_incident` | 1 if incident (diagnosed after baseline) |
| `outcome_status` | Primary disease event indicator (1 = event, 0 = censored, NA = prevalent) |
| `outcome_surv_time` | Follow-up time in years (NA for prevalent cases) |

Prevalent cases of the **primary disease** receive `NA` for both
`outcome_status` and `outcome_surv_time` because they are not at risk for
incident disease. In a Cox model you should exclude or handle them explicitly.

## Case classification logic

The function classifies each participant as follows:

1. **Prevalent case** -- earliest diagnosis date (from `prevalent_sources`) is
   on or before the baseline date. The participant already had the disease at
   enrollment.
2. **Incident case** -- earliest diagnosis date (from `outcome_sources`) is
   after the baseline date. This is a new event during follow-up.
3. **Censored** -- no diagnosis by the end of follow-up (death or
   administrative censor date, whichever comes first).

## Follow-up time

Follow-up time for the primary disease is calculated as:

- **Prevalent case**: `NA` (not at risk).
- **Incident case**: `(diagnosis_date - baseline_date) / 365.25`.
- **Censored**: `(min(death_date, censor_date) - baseline_date) / 365.25`.

## Dual-source design

The `prevalent_sources` and `outcome_sources` arguments let you use different
data sources for baseline exclusion versus outcome ascertainment. This is
recommended because:

| Aspect | Prevalent (history) | Outcome (incident) |
|:-------|:--------------------|:-------------------|
| Purpose | Exclude baseline cases | Define endpoint |
| Self-report | Include (captures pre-existing) | Exclude (imprecise dates) |
| Date precision | Less critical | Critical for survival time |

Default settings reflect this design:

```r
# prevalent_sources includes Self-report
# outcome_sources  excludes Self-report
analysis_dt <- build_survival_dataset(
  dt = ukb_data,
  disease_definitions = diseases,
  prevalent_sources = c("ICD10", "ICD9",
                        "Self-report", "Death"),
  outcome_sources = c("ICD10", "ICD9", "Death"),
  primary_disease = "AA"
)
```

## Running a Cox model

After building the survival dataset, exclude prevalent primary-disease cases
and fit a Cox model:

```r
library(survival)

# Exclude prevalent AA cases (they have NA outcome)
cox_data <- analysis_dt[!is.na(outcome_status)]

cox_model <- coxph(
  Surv(outcome_surv_time, outcome_status) ~
    Hypertension_history + Diabetes_history,
  data = cox_data
)
summary(cox_model)
```

## Sensitivity analyses

You can run sensitivity analyses by varying the source definitions:

```r
# Sensitivity 1: hospital records only (strictest)
strict_dt <- build_survival_dataset(
  ukb_data, diseases,
  prevalent_sources = c("ICD10", "ICD9"),
  outcome_sources   = c("ICD10", "ICD9"),
  primary_disease   = "AA"
)

# Sensitivity 2: all sources including self-report for outcome
broad_dt <- build_survival_dataset(
  ukb_data, diseases,
  prevalent_sources = c("ICD10", "ICD9",
                        "Self-report", "Death"),
  outcome_sources   = c("ICD10", "ICD9",
                        "Self-report", "Death"),
  primary_disease   = "AA"
)
```

## Long format output

If you need a case-level (long) table instead of the wide cohort table,
set `output = "long"`:

```r
long_dt <- build_survival_dataset(
  ukb_data, diseases,
  primary_disease = "AA",
  output = "long",
  include_all = TRUE
)
head(long_dt)
```

<!--chapter:end:04-survival-analysis.Rmd-->

# Baseline Table {#baseline-table}

A Table 1 summarising baseline characteristics stratified by case/control
status is standard in epidemiological papers. UKBAnalytica wraps the
[tableone](https://github.com/kaz-yos/tableone) package for this purpose.

## Basic usage

```r
library(UKBAnalytica)
library(data.table)

# Assume `analysis_dt` was produced by build_survival_dataset()
table1 <- create_baseline_table(
  data = analysis_dt,
  case_col = "outcome_status",
  factor_cols = c(
    "sex", "smoking", "drinking",
    "ethnicity", "education"
  ),
  continuous_cols = c(
    "age", "bmi", "sbp", "dbp",
    "ldl", "hdl", "hba1c"
  ),
  test = TRUE
)

print(table1, smd = TRUE)
```

## Exporting to CSV

The `tableone` print method returns a character matrix that can be written
directly to CSV:

```r
tab_mat <- print(
  table1,
  quote  = TRUE,
  noSpaces = TRUE
)
write.csv(tab_mat, file = "baseline_table1.csv")
```

## Using with the `survival` package example data

If you do not have UKB data at hand, you can test the function with the
`pbc` dataset bundled in the `survival` package:

```r
data(pbc, package = "survival")

table1 <- create_baseline_table(
  data       = pbc,
  case_col   = "trt",
  factor_cols = c("status", "edema", "stage"),
  continuous_cols = c("age", "bili", "albumin"),
  test       = TRUE
)

print(table1)
```

## Parameters

| Argument | Description |
|:---------|:------------|
| `data` | A data.frame or data.table with the cohort |
| `case_col` | Column name for the stratification variable |
| `factor_cols` | Variables to treat as categorical |
| `continuous_cols` | Variables to treat as continuous |
| `test` | Whether to run statistical tests (default `FALSE`) |

<!--chapter:end:05-baseline-table.Rmd-->

# Multiple Imputation {#imputation}

Missing data are common in large cohort studies. UKBAnalytica provides
`run_imputation()`, a wrapper around the
[mice](https://github.com/amices/mice) package that handles the
subset-impute-merge workflow in a single call.

## Motivation

A typical imputation pipeline for UKB data involves:

1. Selecting the covariates to impute.
2. Running `mice::mice()`.
3. Extracting each completed dataset.
4. Merging the imputed covariates back to the full data (exposures, outcomes, etc.).
5. Optionally joining additional omics datasets (proteomics, metabolomics).

`run_imputation()` automates all five steps.

## Basic usage

```r
library(UKBAnalytica)
library(data.table)

dt <- fread("population.csv")

# Variables to impute
imp_vars <- c(
  "sex", "age", "bmi",
  "smoking", "drinking",
  "education", "income",
  "exercise_intensity", "sleep_duration",
  "ethnicity",
  "ldl", "hdl", "hba1c",
  "sbp", "dbp"
)

# Categorical variables (will be coerced to factor)
cat_vars <- c(
  "sex", "smoking", "drinking",
  "education", "income", "ethnicity",
  "exercise_intensity"
)

res <- run_imputation(
  data        = dt,
  id_col      = "eid",
  vars        = imp_vars,
  factor_vars = cat_vars,
  method      = "pmm",
  m           = 5,
  maxit       = 10,
  seed        = 1234,
  print       = FALSE
)
```

## Working with the output

`run_imputation()` returns a list with two elements:

| Element | Description |
|:--------|:------------|
| `imp` | The `mice` `mids` object (for diagnostics, pooling, etc.) |
| `data_list` | A list of `m` completed datasets with imputed covariates merged back |

Access individual datasets:

```r
# First completed dataset
imputed_dt1 <- res$data_list[[1]]
dim(imputed_dt1)

# Second completed dataset
imputed_dt2 <- res$data_list[[2]]
```

All columns that were *not* in `vars` (exposures, outcomes, follow-up time,
etc.) are preserved as-is.

## Merging additional datasets

If you have separate omics datasets (e.g., proteomics, metabolomics), you can
merge them automatically:

```r
pro_df  <- fread("protein_imputed.csv")
meta_df <- fread("metabolomics_imputed.csv")

res <- run_imputation(
  data        = dt,
  id_col      = "eid",
  vars        = imp_vars,
  factor_vars = cat_vars,
  m           = 5,
  seed        = 1234,
  print       = FALSE,
  additional_data = list(
    protein      = pro_df,
    metabolomics = meta_df
  ),
  additional_join = "inner"
)

# Each element of data_list now includes the
# omics columns as well
dim(res$data_list[[1]])
```

Set `additional_join = "left"` if you want to keep all participants even when
they are missing from the omics data.

## Diagnostics

Because the raw `mids` object is returned, you can use standard `mice`
diagnostics:

```r
library(mice)

# Convergence plots
plot(res$imp)

# Strip plots for a specific variable
stripplot(res$imp, bmi ~ .imp)

# Density plots
densityplot(res$imp, ~ bmi)
```

## Pooled analysis

For analyses that account for imputation uncertainty, use `mice::with()` and
`mice::pool()`:

```r
library(mice)
library(survival)

fit <- with(res$imp, lm(bmi ~ age + sex))
pooled <- pool(fit)
summary(pooled)
```

## Saving completed datasets

```r
for (i in seq_along(res$data_list)) {
  saveRDS(
    res$data_list[[i]],
    file = sprintf("imputed_dataset_%d.rds", i)
  )
}
```

## Parameters

| Argument | Default | Description |
|:---------|:--------|:------------|
| `data` | -- | Input data.frame or data.table |
| `id_col` | `"eid"` | Name of the ID column |
| `vars` | -- | Variables to impute |
| `factor_vars` | `NULL` | Subset of `vars` to treat as factors |
| `method` | `"pmm"` | Imputation method (passed to `mice`) |
| `m` | `5` | Number of imputations |
| `maxit` | `10` | Maximum iterations |
| `seed` | `1234` | Random seed |
| `print` | `TRUE` | Show iteration logs |
| `additional_data` | `NULL` | Named list of extra datasets to merge |
| `additional_join` | `"inner"` | Join type for additional datasets |

<!--chapter:end:06-imputation.Rmd-->

# Main Analysis {#main-analysis}

This chapter demonstrates how to perform statistical analyses using **UKBAnalytica**, including pre-analysis correlation checks and three types of regression models: linear regression, logistic regression, and Cox proportional hazards regression.

## Pre-Analysis: Correlation Check {#correlation}

Before running regression models, it's important to examine correlations between variables to identify potential multicollinearity issues and understand relationships in your data.

### Calculate Correlation Matrix

The `run_correlation()` function computes pairwise correlations and highlights values above a specified threshold:

```{r correlation-basic, eval=FALSE}
library(UKBAnalytica)
library(data.table)

# Example data
dt <- data.table(
  age = rnorm(500, 55, 10),
  bmi = rnorm(500, 26, 4),
  sbp = rnorm(500, 130, 15),
  glucose = rnorm(500, 5.5, 1.2),
  cholesterol = rnorm(500, 5.2, 1.0)
)

# Add some correlations to make it more realistic
dt[, sbp := sbp + 0.4 * age + 0.3 * bmi + rnorm(500, 0, 8)]
dt[, glucose := glucose + 0.2 * bmi + rnorm(500, 0, 0.8)]

# Calculate correlation matrix
corr_mat <- run_correlation(
  dt,
  vars = c("age", "bmi", "sbp", "glucose", "cholesterol"),
  method = "pearson",
  threshold = 0.3
)

print(corr_mat)
```

**Parameters:**

- `df`: A data.frame or data.table containing variables
- `vars`: Character vector of variable names to correlate
- `method`: Correlation method (`"pearson"`, `"spearman"`, or `"kendall"`)
- `threshold`: Numeric threshold (0-1) to highlight strong correlations

The function will print correlations exceeding the threshold and return the correlation matrix.

### Visualize Correlation Heatmap

Use `plot_correlation()` to create a customizable heatmap:

```{r correlation-viz, eval=FALSE}
# Basic heatmap with values
p1 <- plot_correlation(
  corr_mat,
  title = "Variable Correlations",
  show_values = TRUE,
  digits = 2
)
print(p1)

# Upper triangle only (cleaner view)
p2 <- plot_correlation(
  corr_mat,
  title = "Correlation Heatmap (Upper Triangle)",
  show_values = TRUE,
  upper_triangle = TRUE,
  text_size = 3.5
)
print(p2)

# Custom color scheme
p3 <- plot_correlation(
  corr_mat,
  title = "Custom Color Correlation Plot",
  color_low = "#2166AC",   # Blue for negative
  color_mid = "#F7F7F7",   # Light gray for zero
  color_high = "#B2182B",  # Red for positive
  show_values = TRUE
)
print(p3)
```

**Key Features:**

- `show_values`: Display correlation coefficients on tiles
- `digits`: Control decimal precision (1-4)
- `text_size`: Adjust label font size
- `color_low/mid/high`: Customize color gradient
- `upper_triangle`: Show only upper half to reduce redundancy

The function automatically adjusts text color (white on strong correlations, black on weak) for optimal readability.

---

## Regression Analysis {#regression}

**UKBAnalytica** provides three regression functions with a unified interface for testing multiple variables:

- `runmulti_lm()`: Linear regression
- `runmulti_logit()`: Logistic regression  
- `runmulti_cox()`: Cox proportional hazards

All functions support both **univariate** (no covariates) and **multivariate** (adjusted) analyses.

### Linear Regression {#lm}

Use `runmulti_lm()` for continuous outcomes:

```{r lm-example, eval=FALSE}
# Create example dataset
set.seed(123)
dt <- data.table(
  sbp = rnorm(500, 130, 15),
  age = rnorm(500, 55, 10),
  bmi = rnorm(500, 26, 4),
  sex = sample(0:1, 500, replace = TRUE),
  smoking = sample(0:1, 500, replace = TRUE)
)

# Add realistic relationships
dt[, sbp := 100 + 0.5 * age + 0.8 * bmi + 3 * smoking + rnorm(500, 0, 8)]

# Univariate linear regression
# Test each variable's association with SBP without adjustment
results_uni <- runmulti_lm(
  data = dt,
  main_var = c("age", "bmi", "sex", "smoking"),
  outcome = "sbp"
)
print(results_uni)

# Multivariate linear regression
# Test each variable adjusting for age and sex
results_multi <- runmulti_lm(
  data = dt,
  main_var = c("bmi", "smoking"),
  covariates = c("age", "sex"),
  outcome = "sbp"
)
print(results_multi)
```

**Output columns:**

- `variable`: Name of the tested variable
- `beta`: Regression coefficient (unit change in outcome per unit change in predictor)
- `lower95`, `upper95`: 95% confidence interval bounds
- `pvalue`: Statistical significance

**Interpretation example:**  
If `beta = 0.8` for BMI, a 1-unit increase in BMI is associated with a 0.8 mmHg increase in systolic blood pressure.

---

### Logistic Regression {#logistic}

Use `runmulti_logit()` for binary (0/1) outcomes:

```{r logit-example, eval=FALSE}
# Create example dataset with binary outcome
set.seed(456)
dt <- data.table(
  diabetes = sample(0:1, 500, replace = TRUE, prob = c(0.85, 0.15)),
  age = rnorm(500, 55, 10),
  bmi = rnorm(500, 26, 4),
  sex = sample(0:1, 500, replace = TRUE),
  family_history = sample(0:1, 500, replace = TRUE, prob = c(0.7, 0.3))
)

# Add realistic risk relationships
dt[, diabetes := rbinom(
  500, 1, 
  plogis(-5 + 0.05 * age + 0.1 * bmi + 0.8 * family_history)
)]

# Univariate logistic regression
results_uni <- runmulti_logit(
  data = dt,
  main_var = c("age", "bmi", "sex", "family_history"),
  outcome = "diabetes"
)
print(results_uni)

# Multivariate logistic regression  
results_multi <- runmulti_logit(
  data = dt,
  main_var = c("bmi", "family_history"),
  covariates = c("age", "sex"),
  outcome = "diabetes"
)
print(results_multi)
```

**Output columns:**

- `variable`: Name of the tested variable
- `OR`: Odds ratio (exponentiated coefficient)
- `lower95`, `upper95`: 95% confidence interval for OR
- `pvalue`: Statistical significance

**Interpretation example:**  
If `OR = 1.5` for family history, individuals with a family history have 1.5 times the odds of diabetes compared to those without.

**Important:** The outcome variable must be binary (0/1). The function will throw an error if other values are detected.

---

### Cox Proportional Hazards Regression {#cox}

Use `runmulti_cox()` for time-to-event (survival) outcomes:

```{r cox-example, eval=FALSE}
library(survival)  # Required for Cox models

# Load example survival data
data(lung)
dt <- as.data.table(lung)

# Clean data
dt <- dt[complete.cases(dt[, .(time, status, age, sex, ph.ecog, wt.loss)])]

# Univariate Cox regression
# Test each variable's effect on survival without adjustment
results_uni <- runmulti_cox(
  data = dt,
  main_var = c("age", "sex", "ph.ecog", "wt.loss"),
  endpoint = c("time", "status")
)
print(results_uni)

# Multivariate Cox regression
# Test variables adjusting for age and sex
results_multi <- runmulti_cox(
  data = dt,
  main_var = c("ph.ecog", "wt.loss"),
  covariates = c("age", "sex"),
  endpoint = c("time", "status")
)
print(results_multi)
```

**Output columns:**

- `variable`: Name of the tested variable
- `HR`: Hazard ratio (instantaneous risk ratio)
- `lower95`, `upper95`: 95% confidence interval for HR
- `pvalue`: Statistical significance

**Interpretation example:**  
If `HR = 1.3` for ECOG performance score, each 1-point increase in ECOG is associated with a 30% increase in the instantaneous hazard (risk) of death.

**Parameters:**

- `endpoint`: A length-2 vector `c("time_var", "status_var")` where:
  - `time_var`: Follow-up time (continuous)
  - `status_var`: Event indicator (1 = event occurred, 0 = censored)

**Note:** The `survival` package must be installed to use `runmulti_cox()`.

---

## Unified Interface Across Models {#unified-interface}

All three regression functions share the same parameter structure:

```{r unified-interface, eval=FALSE}
# General syntax
results <- runmulti_*(
  data = your_data,              # data.frame or data.table
  main_var = c("var1", "var2"),  # Variables to test (one at a time)
  covariates = c("age", "sex"),  # Adjustment variables (NULL = univariate)
  outcome = "y",                 # For lm/logit: outcome variable
  endpoint = c("time", "event")  # For cox: time-to-event variables
)
```

**Key design principles:**

1. **One model per main variable:** Each variable in `main_var` is tested separately
2. **Univariate vs. Multivariate:**  
   - `covariates = NULL` → Univariate analysis (no adjustment)
   - `covariates = c(...)` → Multivariate analysis (adjusted for covariates)
3. **Consistent output:** All functions return a data.frame with effect estimates, confidence intervals, and p-values

---

## Best Practices {#best-practices}

### 1. Check Correlations First

```{r best-practice-1, eval=FALSE}
# Identify multicollinearity before regression
corr_mat <- run_correlation(
  dt, 
  vars = c("var1", "var2", "var3", "var4"),
  threshold = 0.7
)

# If two variables have |r| > 0.8, consider removing one
```

### 2. Start with Univariate, Then Multivariate

```{r best-practice-2, eval=FALSE}
# Step 1: Screen all variables without adjustment
uni_results <- runmulti_lm(
  dt, 
  main_var = c("var1", "var2", "var3"),
  outcome = "y"
)

# Step 2: Test significant variables with adjustment
sig_vars <- uni_results$variable[uni_results$pvalue < 0.05]
multi_results <- runmulti_lm(
  dt,
  main_var = sig_vars,
  covariates = c("age", "sex"),
  outcome = "y"
)
```

### 3. Validate Model Assumptions

```{r best-practice-3, eval=FALSE}
# For linear models: check residuals
model <- lm(y ~ x + age + sex, data = dt)
plot(model)  # Diagnostic plots

# For logistic models: check for complete separation
table(dt$outcome, dt$predictor)

# For Cox models: check proportional hazards assumption
library(survival)
model <- coxph(Surv(time, status) ~ x + age, data = dt)
cox.zph(model)  # Test PH assumption
```

### 4. Handle Missing Data Appropriately

```{r best-practice-4, eval=FALSE}
# Check missingness
colSums(is.na(dt))

# Complete case analysis (default in regression functions)
dt_complete <- dt[complete.cases(dt[, .(outcome, var1, var2, age, sex)])]

# Or use multiple imputation (see Chapter 6)
```

---

## Summary

This chapter covered:

- **Pre-analysis:** Using `run_correlation()` and `plot_correlation()` to examine variable relationships
- **Linear regression:** `runmulti_lm()` for continuous outcomes
- **Logistic regression:** `runmulti_logit()` for binary outcomes  
- **Cox regression:** `runmulti_cox()` for survival outcomes
- **Best practices:** Workflow recommendations for robust analyses

All functions follow a consistent interface, making it easy to switch between analysis types while maintaining reproducible code.

<!--chapter:end:07-main-analysis.Rmd-->

