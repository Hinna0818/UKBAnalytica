[["index.html", "UKBAnalytica: A Guide to UK Biobank Data Analysis Chapter 1 Introduction 1.1 Key features 1.2 Installation 1.3 Package overview 1.4 Core functions", " UKBAnalytica: A Guide to UK Biobank Data Analysis Nan He Department of Bioinformatics, School of Basic Medical Sciences, Southern Medical University hinna@i.smu.edu.cn 2026-02-13 Chapter 1 Introduction UKBAnalytica is a high-performance R package for processing UK Biobank (UKB) Research Analysis Platform (RAP) data exports. It provides efficient extraction of diagnosis records from multiple sources and generates Cox regression-ready survival datasets. 1.1 Key features Built on data.table for efficient processing of large-scale biobank data. Supports ICD-10, ICD-9, self-reported illness, and death registry data. Flexible dual-source case definitions for main and sensitivity analyses. Generates survival datasets with proper prevalent/incident case handling. Includes predefined definitions for common cardiovascular and metabolic diseases. Provides standardized preprocessing for common UKB baseline variables. 1.2 Installation Install the development version from GitHub: # install.packages(&quot;devtools&quot;) devtools::install_github(&quot;Hinna0818/UKBAnalytica&quot;) 1.3 Package overview The package covers the following workflow: Data download – Python scripts for downloading data from the RAP platform. Variable preprocessing – Standardized cleaning and recoding of baseline characteristics. Disease phenotyping – Predefined and custom disease definitions from multiple coding systems. Survival analysis – Build Cox-ready datasets with proper prevalent/incident classification. Baseline tables – Create Table 1 summaries stratified by case/control status. Multiple imputation – Impute missing covariates and merge results back to the full dataset. The remaining chapters walk through each step with code examples. 1.4 Core functions Function Description build_survival_dataset() Build wide-format survival dataset extract_cases_by_source() Extract cases from specified data sources get_predefined_diseases() Get predefined disease definitions create_disease_definition() Create a custom disease definition preprocess_baseline() Preprocess baseline characteristics create_baseline_table() Create a Table 1 (via tableone) run_imputation() Multiple imputation with mice "],["rap-download.html", "Chapter 2 Downloading Data from RAP 2.1 File structure 2.2 Demographic data 2.3 Metabolomics data (NMR) 2.4 Proteomics data (Olink) 2.5 Common UKB field IDs 2.6 Typical output sizes 2.7 Troubleshooting", " Chapter 2 Downloading Data from RAP The UKBAnalytica package ships Python helper scripts for downloading data from the UK Biobank Research Analysis Platform (RAP). These scripts live in inst/python/ after package installation. 2.1 File structure inst/ python/ ukb_data_loader.py # Demographics &amp; metabolites (Spark) protein_loader.py # Proteomics (dx commands) field_ids_demographic.txt # Example demographic field IDs extdata/ metabolites_non_ratio.txt # Non-ratio metabolite reference (170 fields) 2.2 Demographic data Download any combination of UKB fields by specifying their field IDs. The loader uses Spark via dxdata under the hood. # Pass IDs directly python ukb_data_loader.py demographic \\ --ids 31,53,21022,21001 \\ -o population.csv # Or read IDs from a file (recommended for many fields) python ukb_data_loader.py demographic \\ --id-file field_ids_demographic.txt \\ -o population.csv The ID file supports comments (#), comma-separated and space-separated formats: # Demographics 31 # Sex 53 # Date of assessment 21022 # Age at recruitment # Comma separated 20116, 20117, 1289 2.3 Metabolomics data (NMR) Download NMR metabolomics data. You can retrieve all 251 metabolite fields or restrict to the curated non-ratio subset of 170 fields. # All metabolite fields python ukb_data_loader.py metabolites -o metabolites_all.csv # Non-ratio subset only python ukb_data_loader.py metabolites \\ --non-ratio \\ -o metabolites_non_ratio.csv 2.4 Proteomics data (Olink) Download Olink protein expression data via dx commands. The loader handles batching, merging, and progress tracking automatically. # Default settings python protein_loader.py -o ./protein_data # Custom batch size and delay python protein_loader.py \\ -o ./protein_data \\ --batch-size 100 \\ --delay 3 # Skip merging batch files python protein_loader.py -o ./protein_data --no-merge 2.5 Common UKB field IDs The table below lists commonly used field IDs for reference: Category Field IDs Description Basic demographics 31, 53, 21022, 21001 Sex, assessment date, age, BMI Lifestyle 20116, 20117, 1160 Smoking, alcohol, sleep Blood pressure 93, 94, 4079, 4080 Systolic/diastolic BP Biomarkers 30870, 30780, 30760, 30750 Triglycerides, LDL, HDL, glucose Hospital records 41270, 41280, 41271, 41281 ICD-10/9 diagnoses + dates Death registry 40000, 40001, 40002 Death date, causes 2.6 Typical output sizes Data type File Approx. size Rows Columns Demographics population.csv 10–500 MB ~500 K variable Metabolites (all) metabolites.csv ~300 MB ~120 K 251 Metabolites (non-ratio) metabolites_non_ratio.csv ~200 MB ~120 K 170 Proteomics protein_all_merged.csv ~800 MB ~50 K ~3000 2.7 Troubleshooting Spark initialisation fails: make sure you are running inside a RAP JupyterLab session. Field not found: some fields require instance/array suffixes (e.g., _i0, _a0). Memory issues: reduce --batch-size for protein downloads. Timeout: increase --delay. "],["preprocessing.html", "Chapter 3 Variable Preprocessing 3.1 Available variables 3.2 Basic usage 3.3 Blood pressure 3.4 Medication use 3.5 Air pollution exposure 3.6 Custom variable mapping 3.7 Complete preprocessing example", " Chapter 3 Variable Preprocessing After downloading data from RAP, the next step is to clean and recode baseline variables. UKBAnalytica provides preprocess_baseline() and several helper functions to handle this in a standardised way. 3.1 Available variables Use get_variable_info() to inspect the built-in variable catalogue: library(UKBAnalytica) # View all variables in a category get_variable_info(&quot;demographics&quot;) get_variable_info(&quot;lifestyle&quot;) get_variable_info(&quot;biomarkers&quot;) Each entry maps a human-readable name (e.g., \"bmi\") to the corresponding UKB field column (e.g., p21001_i0). 3.2 Basic usage preprocess_baseline() selects, renames, and recodes the requested variables: library(UKBAnalytica) library(data.table) ukb_data &lt;- fread(&quot;population.csv&quot;) processed &lt;- preprocess_baseline( ukb_data, variables = c(&quot;sex&quot;, &quot;age&quot;, &quot;ethnicity&quot;, &quot;bmi&quot;, &quot;smoking&quot;, &quot;education&quot;), missing_action = &quot;keep&quot; ) head(processed) Set missing_action = \"drop\" to remove any row that has NA in the selected variables. 3.3 Blood pressure UKB records automated and manual BP readings. The helper calculate_blood_pressure() averages available readings per participant: processed &lt;- calculate_blood_pressure(processed, type = &quot;sbp&quot;) processed &lt;- calculate_blood_pressure(processed, type = &quot;dbp&quot;) This will create columns sbp and dbp from whichever readings are present (automated preferred, manual as fallback). 3.4 Medication use Extract binary indicators for medication use at baseline: processed &lt;- extract_medications( processed, c(&quot;cholesterol&quot;, &quot;blood_pressure&quot;, &quot;insulin&quot;) ) The resulting columns are named anti_cho, antihypertensive, and insulin (1 = taking medication, 0 = not). 3.5 Air pollution exposure Calculate averaged air pollution exposures from the UKB environmental data: processed &lt;- calculate_air_pollution( processed, c(&quot;NO2&quot;, &quot;PM2.5&quot;, &quot;PM10&quot;, &quot;NOx&quot;) ) 3.6 Custom variable mapping If you need a UKB field that is not in the built-in catalogue, pass a custom_mapping list: custom &lt;- list( my_biomarker = list( ukb_col = &quot;p30000_i0&quot;, description = &quot;Custom biomarker&quot; ) ) processed &lt;- preprocess_baseline( ukb_data, variables = c(&quot;sex&quot;, &quot;age&quot;, &quot;my_biomarker&quot;), custom_mapping = custom ) The custom entry will be treated like any built-in variable – the column is renamed and included in the output. 3.7 Complete preprocessing example A typical preprocessing pipeline for a cardiovascular study might look like this: library(UKBAnalytica) library(data.table) dt &lt;- fread(&quot;population.csv&quot;) # Step 1: demographics + lifestyle + biomarkers dt &lt;- preprocess_baseline( dt, variables = c( &quot;sex&quot;, &quot;age&quot;, &quot;ethnicity&quot;, &quot;bmi&quot;, &quot;smoking&quot;, &quot;drinking&quot;, &quot;sleep_duration&quot;, &quot;exercise_intensity&quot;, &quot;education&quot;, &quot;income&quot;, &quot;triglycerides&quot;, &quot;ldl&quot;, &quot;hdl&quot;, &quot;hba1c&quot; ) ) # Step 2: blood pressure dt &lt;- calculate_blood_pressure(dt, &quot;sbp&quot;) dt &lt;- calculate_blood_pressure(dt, &quot;dbp&quot;) # Step 3: medications dt &lt;- extract_medications( dt, c(&quot;cholesterol&quot;, &quot;blood_pressure&quot;, &quot;insulin&quot;) ) # Step 4: air pollution dt &lt;- calculate_air_pollution(dt, c(&quot;NO2&quot;, &quot;PM2.5&quot;)) str(dt) "],["disease-definitions.html", "Chapter 4 Disease Definitions 4.1 Predefined diseases 4.2 Inspecting a definition 4.3 Creating custom definitions 4.4 Composite endpoints 4.5 Comparing data sources", " Chapter 4 Disease Definitions UKBAnalytica identifies disease cases from up to four data sources: Source Code field Date field ICD-10 (hospital) p41270 p41280_a* ICD-9 (hospital) p41271 p41281_a* Self-report p20002_i*_a* p20008_i*_a* Death registry p40001, p40002 p40000 4.1 Predefined diseases The package ships a curated library of definitions for common cardiovascular and metabolic conditions. Retrieve them with get_predefined_diseases(): library(UKBAnalytica) diseases &lt;- get_predefined_diseases() names(diseases) Select a subset by name: my_diseases &lt;- get_predefined_diseases()[ c(&quot;AA&quot;, &quot;Hypertension&quot;, &quot;Diabetes&quot;) ] Each definition is a list specifying ICD-10 codes, ICD-9 codes, self-report codes, and death-cause codes. 4.2 Inspecting a definition diseases &lt;- get_predefined_diseases() str(diseases[[&quot;Hypertension&quot;]]) A definition typically contains: icd10 – character vector of ICD-10 codes (prefix-matched). icd9 – character vector of ICD-9 codes. self_report – integer vector of self-report coding IDs. death_icd10 – character vector of death-cause ICD-10 codes. 4.3 Creating custom definitions Use create_disease_definition() to define a new disease: my_disease &lt;- create_disease_definition( icd10 = c(&quot;K70&quot;, &quot;K71&quot;, &quot;K72&quot;, &quot;K73&quot;, &quot;K74&quot;), icd9 = c(&quot;571&quot;), self_report = c(1604), death_icd10 = c(&quot;K70&quot;, &quot;K74&quot;) ) You can combine it with predefined diseases: all_diseases &lt;- c( get_predefined_diseases()[c(&quot;Hypertension&quot;, &quot;Diabetes&quot;)], list(LiverDisease = my_disease) ) 4.4 Composite endpoints Create composite endpoints by merging multiple definitions: mace &lt;- combine_disease_definitions( get_predefined_diseases()[c(&quot;MI&quot;, &quot;Stroke&quot;, &quot;HF&quot;)] ) This merges ICD-10, ICD-9, self-report, and death codes from all input definitions into a single combined definition. 4.5 Comparing data sources To check how many cases each source contributes, use compare_data_sources(): library(data.table) ukb_data &lt;- fread(&quot;population.csv&quot;) diseases &lt;- get_predefined_diseases()[c(&quot;AA&quot;, &quot;Hypertension&quot;)] comparison &lt;- compare_data_sources(ukb_data, diseases) comparison This returns a summary table showing case counts per source per disease, which is useful for justifying source selection in your manuscript. "],["survival-analysis.html", "Chapter 5 Survival Analysis 5.1 Quick start 5.2 Output columns 5.3 Case classification logic 5.4 Follow-up time 5.5 Dual-source design 5.6 Running a Cox model 5.7 Sensitivity analyses 5.8 Long format output", " Chapter 5 Survival Analysis The main function for building a Cox-regression-ready dataset is build_survival_dataset(). It handles prevalent/incident classification, follow-up time calculation, and supports separate source definitions for baseline exclusion and outcome ascertainment. 5.1 Quick start library(UKBAnalytica) library(data.table) ukb_data &lt;- fread(&quot;population.csv&quot;) diseases &lt;- get_predefined_diseases()[ c(&quot;AA&quot;, &quot;Hypertension&quot;, &quot;Diabetes&quot;) ] analysis_dt &lt;- build_survival_dataset( dt = ukb_data, disease_definitions = diseases, primary_disease = &quot;AA&quot;, censor_date = as.Date(&quot;2023-12-31&quot;) ) head(analysis_dt[, .(eid, AA_history, AA_incident, Hypertension_history, Diabetes_history, outcome_status, outcome_surv_time)]) 5.2 Output columns The wide-format output contains the following columns for each disease: Column Description {Disease}_history 1 if prevalent (diagnosed at or before baseline) {Disease}_incident 1 if incident (diagnosed after baseline) outcome_status Primary disease event indicator (1 = event, 0 = censored, NA = prevalent) outcome_surv_time Follow-up time in years (NA for prevalent cases) Prevalent cases of the primary disease receive NA for both outcome_status and outcome_surv_time because they are not at risk for incident disease. In a Cox model you should exclude or handle them explicitly. 5.3 Case classification logic The function classifies each participant as follows: Prevalent case – earliest diagnosis date (from prevalent_sources) is on or before the baseline date. The participant already had the disease at enrollment. Incident case – earliest diagnosis date (from outcome_sources) is after the baseline date. This is a new event during follow-up. Censored – no diagnosis by the end of follow-up (death or administrative censor date, whichever comes first). 5.4 Follow-up time Follow-up time for the primary disease is calculated as: Prevalent case: NA (not at risk). Incident case: (diagnosis_date - baseline_date) / 365.25. Censored: (min(death_date, censor_date) - baseline_date) / 365.25. 5.5 Dual-source design The prevalent_sources and outcome_sources arguments let you use different data sources for baseline exclusion versus outcome ascertainment. This is recommended because: Aspect Prevalent (history) Outcome (incident) Purpose Exclude baseline cases Define endpoint Self-report Include (captures pre-existing) Exclude (imprecise dates) Date precision Less critical Critical for survival time Default settings reflect this design: # prevalent_sources includes Self-report # outcome_sources excludes Self-report analysis_dt &lt;- build_survival_dataset( dt = ukb_data, disease_definitions = diseases, prevalent_sources = c(&quot;ICD10&quot;, &quot;ICD9&quot;, &quot;Self-report&quot;, &quot;Death&quot;), outcome_sources = c(&quot;ICD10&quot;, &quot;ICD9&quot;, &quot;Death&quot;), primary_disease = &quot;AA&quot; ) 5.6 Running a Cox model After building the survival dataset, exclude prevalent primary-disease cases and fit a Cox model: library(survival) # Exclude prevalent AA cases (they have NA outcome) cox_data &lt;- analysis_dt[!is.na(outcome_status)] cox_model &lt;- coxph( Surv(outcome_surv_time, outcome_status) ~ Hypertension_history + Diabetes_history, data = cox_data ) summary(cox_model) 5.7 Sensitivity analyses You can run sensitivity analyses by varying the source definitions: # Sensitivity 1: hospital records only (strictest) strict_dt &lt;- build_survival_dataset( ukb_data, diseases, prevalent_sources = c(&quot;ICD10&quot;, &quot;ICD9&quot;), outcome_sources = c(&quot;ICD10&quot;, &quot;ICD9&quot;), primary_disease = &quot;AA&quot; ) # Sensitivity 2: all sources including self-report for outcome broad_dt &lt;- build_survival_dataset( ukb_data, diseases, prevalent_sources = c(&quot;ICD10&quot;, &quot;ICD9&quot;, &quot;Self-report&quot;, &quot;Death&quot;), outcome_sources = c(&quot;ICD10&quot;, &quot;ICD9&quot;, &quot;Self-report&quot;, &quot;Death&quot;), primary_disease = &quot;AA&quot; ) 5.8 Long format output If you need a case-level (long) table instead of the wide cohort table, set output = \"long\": long_dt &lt;- build_survival_dataset( ukb_data, diseases, primary_disease = &quot;AA&quot;, output = &quot;long&quot;, include_all = TRUE ) head(long_dt) "],["baseline-table.html", "Chapter 6 Baseline Table 6.1 Basic usage 6.2 Exporting to CSV 6.3 Using with the survival package example data 6.4 Parameters", " Chapter 6 Baseline Table A Table 1 summarising baseline characteristics stratified by case/control status is standard in epidemiological papers. UKBAnalytica wraps the tableone package for this purpose. 6.1 Basic usage library(UKBAnalytica) library(data.table) # Assume `analysis_dt` was produced by build_survival_dataset() table1 &lt;- create_baseline_table( data = analysis_dt, case_col = &quot;outcome_status&quot;, factor_cols = c( &quot;sex&quot;, &quot;smoking&quot;, &quot;drinking&quot;, &quot;ethnicity&quot;, &quot;education&quot; ), continuous_cols = c( &quot;age&quot;, &quot;bmi&quot;, &quot;sbp&quot;, &quot;dbp&quot;, &quot;ldl&quot;, &quot;hdl&quot;, &quot;hba1c&quot; ), test = TRUE ) print(table1, smd = TRUE) 6.2 Exporting to CSV The tableone print method returns a character matrix that can be written directly to CSV: tab_mat &lt;- print( table1, quote = TRUE, noSpaces = TRUE ) write.csv(tab_mat, file = &quot;baseline_table1.csv&quot;) 6.3 Using with the survival package example data If you do not have UKB data at hand, you can test the function with the pbc dataset bundled in the survival package: data(pbc, package = &quot;survival&quot;) table1 &lt;- create_baseline_table( data = pbc, case_col = &quot;trt&quot;, factor_cols = c(&quot;status&quot;, &quot;edema&quot;, &quot;stage&quot;), continuous_cols = c(&quot;age&quot;, &quot;bili&quot;, &quot;albumin&quot;), test = TRUE ) print(table1) 6.4 Parameters Argument Description data A data.frame or data.table with the cohort case_col Column name for the stratification variable factor_cols Variables to treat as categorical continuous_cols Variables to treat as continuous test Whether to run statistical tests (default FALSE) "],["imputation.html", "Chapter 7 Multiple Imputation 7.1 Motivation 7.2 Basic usage 7.3 Working with the output 7.4 Merging additional datasets 7.5 Diagnostics 7.6 Pooled analysis 7.7 Saving completed datasets 7.8 Parameters", " Chapter 7 Multiple Imputation Missing data are common in large cohort studies. UKBAnalytica provides run_imputation(), a wrapper around the mice package that handles the subset-impute-merge workflow in a single call. 7.1 Motivation A typical imputation pipeline for UKB data involves: Selecting the covariates to impute. Running mice::mice(). Extracting each completed dataset. Merging the imputed covariates back to the full data (exposures, outcomes, etc.). Optionally joining additional omics datasets (proteomics, metabolomics). run_imputation() automates all five steps. 7.2 Basic usage library(UKBAnalytica) library(data.table) dt &lt;- fread(&quot;population.csv&quot;) # Variables to impute imp_vars &lt;- c( &quot;sex&quot;, &quot;age&quot;, &quot;bmi&quot;, &quot;smoking&quot;, &quot;drinking&quot;, &quot;education&quot;, &quot;income&quot;, &quot;exercise_intensity&quot;, &quot;sleep_duration&quot;, &quot;ethnicity&quot;, &quot;ldl&quot;, &quot;hdl&quot;, &quot;hba1c&quot;, &quot;sbp&quot;, &quot;dbp&quot; ) # Categorical variables (will be coerced to factor) cat_vars &lt;- c( &quot;sex&quot;, &quot;smoking&quot;, &quot;drinking&quot;, &quot;education&quot;, &quot;income&quot;, &quot;ethnicity&quot;, &quot;exercise_intensity&quot; ) res &lt;- run_imputation( data = dt, id_col = &quot;eid&quot;, vars = imp_vars, factor_vars = cat_vars, method = &quot;pmm&quot;, m = 5, maxit = 10, seed = 1234, print = FALSE ) 7.3 Working with the output run_imputation() returns a list with two elements: Element Description imp The mice mids object (for diagnostics, pooling, etc.) data_list A list of m completed datasets with imputed covariates merged back Access individual datasets: # First completed dataset imputed_dt1 &lt;- res$data_list[[1]] dim(imputed_dt1) # Second completed dataset imputed_dt2 &lt;- res$data_list[[2]] All columns that were not in vars (exposures, outcomes, follow-up time, etc.) are preserved as-is. 7.4 Merging additional datasets If you have separate omics datasets (e.g., proteomics, metabolomics), you can merge them automatically: pro_df &lt;- fread(&quot;protein_imputed.csv&quot;) meta_df &lt;- fread(&quot;metabolomics_imputed.csv&quot;) res &lt;- run_imputation( data = dt, id_col = &quot;eid&quot;, vars = imp_vars, factor_vars = cat_vars, m = 5, seed = 1234, print = FALSE, additional_data = list( protein = pro_df, metabolomics = meta_df ), additional_join = &quot;inner&quot; ) # Each element of data_list now includes the # omics columns as well dim(res$data_list[[1]]) Set additional_join = \"left\" if you want to keep all participants even when they are missing from the omics data. 7.5 Diagnostics Because the raw mids object is returned, you can use standard mice diagnostics: library(mice) # Convergence plots plot(res$imp) # Strip plots for a specific variable stripplot(res$imp, bmi ~ .imp) # Density plots densityplot(res$imp, ~ bmi) 7.6 Pooled analysis For analyses that account for imputation uncertainty, use mice::with() and mice::pool(): library(mice) library(survival) fit &lt;- with(res$imp, lm(bmi ~ age + sex)) pooled &lt;- pool(fit) summary(pooled) 7.7 Saving completed datasets for (i in seq_along(res$data_list)) { saveRDS( res$data_list[[i]], file = sprintf(&quot;imputed_dataset_%d.rds&quot;, i) ) } 7.8 Parameters Argument Default Description data – Input data.frame or data.table id_col \"eid\" Name of the ID column vars – Variables to impute factor_vars NULL Subset of vars to treat as factors method \"pmm\" Imputation method (passed to mice) m 5 Number of imputations maxit 10 Maximum iterations seed 1234 Random seed print TRUE Show iteration logs additional_data NULL Named list of extra datasets to merge additional_join \"inner\" Join type for additional datasets "],["main-analysis.html", "Chapter 8 Main Analysis 8.1 Pre-Analysis: Correlation Check 8.2 Regression Analysis 8.3 Unified Interface Across Models 8.4 Best Practices 8.5 Summary", " Chapter 8 Main Analysis This chapter demonstrates how to perform statistical analyses using UKBAnalytica, including pre-analysis correlation checks and three types of regression models: linear regression, logistic regression, and Cox proportional hazards regression. 8.1 Pre-Analysis: Correlation Check Before running regression models, it’s important to examine correlations between variables to identify potential multicollinearity issues and understand relationships in your data. 8.1.1 Calculate Correlation Matrix The run_correlation() function computes pairwise correlations and highlights values above a specified threshold: library(UKBAnalytica) library(data.table) # Example data dt &lt;- data.table( age = rnorm(500, 55, 10), bmi = rnorm(500, 26, 4), sbp = rnorm(500, 130, 15), glucose = rnorm(500, 5.5, 1.2), cholesterol = rnorm(500, 5.2, 1.0) ) # Add some correlations to make it more realistic dt[, sbp := sbp + 0.4 * age + 0.3 * bmi + rnorm(500, 0, 8)] dt[, glucose := glucose + 0.2 * bmi + rnorm(500, 0, 0.8)] # Calculate correlation matrix corr_mat &lt;- run_correlation( dt, vars = c(&quot;age&quot;, &quot;bmi&quot;, &quot;sbp&quot;, &quot;glucose&quot;, &quot;cholesterol&quot;), method = &quot;pearson&quot;, threshold = 0.3 ) print(corr_mat) Parameters: df: A data.frame or data.table containing variables vars: Character vector of variable names to correlate method: Correlation method (\"pearson\", \"spearman\", or \"kendall\") threshold: Numeric threshold (0-1) to highlight strong correlations The function will print correlations exceeding the threshold and return the correlation matrix. 8.1.2 Visualize Correlation Heatmap Use plot_correlation() to create a customizable heatmap: # Basic heatmap with values p1 &lt;- plot_correlation( corr_mat, title = &quot;Variable Correlations&quot;, show_values = TRUE, digits = 2 ) print(p1) # Upper triangle only (cleaner view) p2 &lt;- plot_correlation( corr_mat, title = &quot;Correlation Heatmap (Upper Triangle)&quot;, show_values = TRUE, upper_triangle = TRUE, text_size = 3.5 ) print(p2) # Custom color scheme p3 &lt;- plot_correlation( corr_mat, title = &quot;Custom Color Correlation Plot&quot;, color_low = &quot;#2166AC&quot;, # Blue for negative color_mid = &quot;#F7F7F7&quot;, # Light gray for zero color_high = &quot;#B2182B&quot;, # Red for positive show_values = TRUE ) print(p3) Key Features: show_values: Display correlation coefficients on tiles digits: Control decimal precision (1-4) text_size: Adjust label font size color_low/mid/high: Customize color gradient upper_triangle: Show only upper half to reduce redundancy The function automatically adjusts text color (white on strong correlations, black on weak) for optimal readability. 8.2 Regression Analysis UKBAnalytica provides three regression functions with a unified interface for testing multiple variables: runmulti_lm(): Linear regression runmulti_logit(): Logistic regression runmulti_cox(): Cox proportional hazards All functions support both univariate (no covariates) and multivariate (adjusted) analyses. 8.2.1 Linear Regression Use runmulti_lm() for continuous outcomes: # Create example dataset set.seed(123) dt &lt;- data.table( sbp = rnorm(500, 130, 15), age = rnorm(500, 55, 10), bmi = rnorm(500, 26, 4), sex = sample(0:1, 500, replace = TRUE), smoking = sample(0:1, 500, replace = TRUE) ) # Add realistic relationships dt[, sbp := 100 + 0.5 * age + 0.8 * bmi + 3 * smoking + rnorm(500, 0, 8)] # Univariate linear regression # Test each variable&#39;s association with SBP without adjustment results_uni &lt;- runmulti_lm( data = dt, main_var = c(&quot;age&quot;, &quot;bmi&quot;, &quot;sex&quot;, &quot;smoking&quot;), outcome = &quot;sbp&quot; ) print(results_uni) # Multivariate linear regression # Test each variable adjusting for age and sex results_multi &lt;- runmulti_lm( data = dt, main_var = c(&quot;bmi&quot;, &quot;smoking&quot;), covariates = c(&quot;age&quot;, &quot;sex&quot;), outcome = &quot;sbp&quot; ) print(results_multi) Output columns: variable: Name of the tested variable beta: Regression coefficient (unit change in outcome per unit change in predictor) lower95, upper95: 95% confidence interval bounds pvalue: Statistical significance Interpretation example: If beta = 0.8 for BMI, a 1-unit increase in BMI is associated with a 0.8 mmHg increase in systolic blood pressure. 8.2.2 Logistic Regression Use runmulti_logit() for binary (0/1) outcomes: # Create example dataset with binary outcome set.seed(456) dt &lt;- data.table( diabetes = sample(0:1, 500, replace = TRUE, prob = c(0.85, 0.15)), age = rnorm(500, 55, 10), bmi = rnorm(500, 26, 4), sex = sample(0:1, 500, replace = TRUE), family_history = sample(0:1, 500, replace = TRUE, prob = c(0.7, 0.3)) ) # Add realistic risk relationships dt[, diabetes := rbinom( 500, 1, plogis(-5 + 0.05 * age + 0.1 * bmi + 0.8 * family_history) )] # Univariate logistic regression results_uni &lt;- runmulti_logit( data = dt, main_var = c(&quot;age&quot;, &quot;bmi&quot;, &quot;sex&quot;, &quot;family_history&quot;), outcome = &quot;diabetes&quot; ) print(results_uni) # Multivariate logistic regression results_multi &lt;- runmulti_logit( data = dt, main_var = c(&quot;bmi&quot;, &quot;family_history&quot;), covariates = c(&quot;age&quot;, &quot;sex&quot;), outcome = &quot;diabetes&quot; ) print(results_multi) Output columns: variable: Name of the tested variable OR: Odds ratio (exponentiated coefficient) lower95, upper95: 95% confidence interval for OR pvalue: Statistical significance Interpretation example: If OR = 1.5 for family history, individuals with a family history have 1.5 times the odds of diabetes compared to those without. Important: The outcome variable must be binary (0/1). The function will throw an error if other values are detected. 8.2.3 Cox Proportional Hazards Regression Use runmulti_cox() for time-to-event (survival) outcomes: library(survival) # Required for Cox models # Load example survival data data(lung) dt &lt;- as.data.table(lung) # Clean data dt &lt;- dt[complete.cases(dt[, .(time, status, age, sex, ph.ecog, wt.loss)])] # Univariate Cox regression # Test each variable&#39;s effect on survival without adjustment results_uni &lt;- runmulti_cox( data = dt, main_var = c(&quot;age&quot;, &quot;sex&quot;, &quot;ph.ecog&quot;, &quot;wt.loss&quot;), endpoint = c(&quot;time&quot;, &quot;status&quot;) ) print(results_uni) # Multivariate Cox regression # Test variables adjusting for age and sex results_multi &lt;- runmulti_cox( data = dt, main_var = c(&quot;ph.ecog&quot;, &quot;wt.loss&quot;), covariates = c(&quot;age&quot;, &quot;sex&quot;), endpoint = c(&quot;time&quot;, &quot;status&quot;) ) print(results_multi) Output columns: variable: Name of the tested variable HR: Hazard ratio (instantaneous risk ratio) lower95, upper95: 95% confidence interval for HR pvalue: Statistical significance Interpretation example: If HR = 1.3 for ECOG performance score, each 1-point increase in ECOG is associated with a 30% increase in the instantaneous hazard (risk) of death. Parameters: endpoint: A length-2 vector c(\"time_var\", \"status_var\") where: time_var: Follow-up time (continuous) status_var: Event indicator (1 = event occurred, 0 = censored) Note: The survival package must be installed to use runmulti_cox(). 8.3 Unified Interface Across Models All three regression functions share the same parameter structure: # General syntax results &lt;- runmulti_*( data = your_data, # data.frame or data.table main_var = c(&quot;var1&quot;, &quot;var2&quot;), # Variables to test (one at a time) covariates = c(&quot;age&quot;, &quot;sex&quot;), # Adjustment variables (NULL = univariate) outcome = &quot;y&quot;, # For lm/logit: outcome variable endpoint = c(&quot;time&quot;, &quot;event&quot;) # For cox: time-to-event variables ) Key design principles: One model per main variable: Each variable in main_var is tested separately Univariate vs. Multivariate: covariates = NULL → Univariate analysis (no adjustment) covariates = c(...) → Multivariate analysis (adjusted for covariates) Consistent output: All functions return a data.frame with effect estimates, confidence intervals, and p-values 8.4 Best Practices 8.4.1 1. Check Correlations First # Identify multicollinearity before regression corr_mat &lt;- run_correlation( dt, vars = c(&quot;var1&quot;, &quot;var2&quot;, &quot;var3&quot;, &quot;var4&quot;), threshold = 0.7 ) # If two variables have |r| &gt; 0.8, consider removing one 8.4.2 2. Start with Univariate, Then Multivariate # Step 1: Screen all variables without adjustment uni_results &lt;- runmulti_lm( dt, main_var = c(&quot;var1&quot;, &quot;var2&quot;, &quot;var3&quot;), outcome = &quot;y&quot; ) # Step 2: Test significant variables with adjustment sig_vars &lt;- uni_results$variable[uni_results$pvalue &lt; 0.05] multi_results &lt;- runmulti_lm( dt, main_var = sig_vars, covariates = c(&quot;age&quot;, &quot;sex&quot;), outcome = &quot;y&quot; ) 8.4.3 3. Validate Model Assumptions # For linear models: check residuals model &lt;- lm(y ~ x + age + sex, data = dt) plot(model) # Diagnostic plots # For logistic models: check for complete separation table(dt$outcome, dt$predictor) # For Cox models: check proportional hazards assumption library(survival) model &lt;- coxph(Surv(time, status) ~ x + age, data = dt) cox.zph(model) # Test PH assumption 8.4.4 4. Handle Missing Data Appropriately # Check missingness colSums(is.na(dt)) # Complete case analysis (default in regression functions) dt_complete &lt;- dt[complete.cases(dt[, .(outcome, var1, var2, age, sex)])] # Or use multiple imputation (see Chapter 6) 8.5 Summary This chapter covered: Pre-analysis: Using run_correlation() and plot_correlation() to examine variable relationships Linear regression: runmulti_lm() for continuous outcomes Logistic regression: runmulti_logit() for binary outcomes Cox regression: runmulti_cox() for survival outcomes Best practices: Workflow recommendations for robust analyses All functions follow a consistent interface, making it easy to switch between analysis types while maintaining reproducible code. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
